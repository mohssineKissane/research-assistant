{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1.5: Embeddings Generator Test\n",
    "\n",
    "**Goal**: Convert text chunks to vector embeddings using HuggingFace models\n",
    "\n",
    "**File**: `src/processing/embeddings.py`\n",
    "\n",
    "This notebook tests the `EmbeddingsGenerator` class which:\n",
    "- Uses free HuggingFace models (runs locally, no API calls)\n",
    "- Converts text to numerical vectors\n",
    "- Enables semantic similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.processing.embeddings import EmbeddingsGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Initialize Embeddings Generator\n",
    "\n",
    "Create an instance using the default model: `sentence-transformers/all-MiniLM-L6-v2`\n",
    "- Fast and lightweight\n",
    "- Good for general-purpose semantic search\n",
    "- Produces 384-dimensional vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing EmbeddingsGenerator...\n",
      "✓ Embeddings model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing EmbeddingsGenerator...\")\n",
    "embedder = EmbeddingsGenerator()\n",
    "print(\"✓ Embeddings model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Embed a Single Query\n",
    "\n",
    "Test the `embed_query()` method with a sample question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What is climate change?'\n",
      "\n",
      "✓ Vector dimension: 384\n",
      "✓ First 5 values: [-0.037313830107450485, 0.09820153564214706, 0.0566871240735054, 0.06354967504739761, 0.03308780491352081]\n",
      "✓ Last 5 values: [0.046277157962322235, -0.033581286668777466, -0.04362424835562706, -0.00490030599758029, 0.0240323543548584]\n",
      "\n",
      "Vector type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "query = \"What is climate change?\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "query_vector = embedder.embed_query(query)\n",
    "\n",
    "print(f\"✓ Vector dimension: {len(query_vector)}\")\n",
    "print(f\"✓ First 5 values: {query_vector[:5]}\")\n",
    "print(f\"✓ Last 5 values: {query_vector[-5:]}\")\n",
    "print(f\"\\nVector type: {type(query_vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Embed Multiple Documents\n",
    "\n",
    "Test the `embed_documents()` method with multiple text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding multiple documents...\n",
      "\n",
      "✓ Number of documents embedded: 4\n",
      "✓ Each vector dimension: 384\n",
      "\n",
      "First document vector (first 5 values): [-0.02915601246058941, 0.019391369074583054, 0.13551342487335205, 0.05121537297964096, 0.02877393178641796]\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"Climate change refers to long-term shifts in global temperatures and weather patterns.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"The greenhouse effect is the warming of Earth's surface and lower atmosphere.\",\n",
    "    \"Python is a popular programming language for data science.\"\n",
    "]\n",
    "\n",
    "print(\"Embedding multiple documents...\\n\")\n",
    "doc_vectors = embedder.embed_documents(documents)\n",
    "\n",
    "print(f\"✓ Number of documents embedded: {len(doc_vectors)}\")\n",
    "print(f\"✓ Each vector dimension: {len(doc_vectors[0])}\")\n",
    "print(f\"\\nFirst document vector (first 5 values): {doc_vectors[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Semantic Similarity Demonstration\n",
    "\n",
    "Calculate cosine similarity between query and documents to show semantic understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'Tell me about global warming'\n",
      "\n",
      "Similarity scores with documents:\n",
      "\n",
      "1. [0.4786] Climate change refers to long-term shifts in global temperat...\n",
      "2. [0.1482] Machine learning is a subset of artificial intelligence....\n",
      "3. [0.5693] The greenhouse effect is the warming of Earth's surface and ...\n",
      "4. [0.1785] Python is a popular programming language for data science....\n",
      "\n",
      "✓ Notice: Climate-related documents have higher similarity scores!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Query about climate\n",
    "climate_query = \"Tell me about global warming\"\n",
    "climate_vector = embedder.embed_query(climate_query)\n",
    "\n",
    "print(f\"Query: '{climate_query}'\\n\")\n",
    "print(\"Similarity scores with documents:\\n\")\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    similarity = cosine_similarity(climate_vector, doc_vectors[i])\n",
    "    print(f\"{i+1}. [{similarity:.4f}] {doc[:60]}...\")\n",
    "\n",
    "print(\"\\n✓ Notice: Climate-related documents have higher similarity scores!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Different Queries Comparison\n",
    "\n",
    "Compare how different queries relate to the same set of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing different queries:\n",
      "\n",
      "\n",
      "Query: 'What causes global warming?'\n",
      "------------------------------------------------------------\n",
      "Best match: The greenhouse effect is the warming of Earth's surface and ...\n",
      "Similarity: 0.6267\n",
      "\n",
      "Query: 'How does AI work?'\n",
      "------------------------------------------------------------\n",
      "Best match: Machine learning is a subset of artificial intelligence....\n",
      "Similarity: 0.5104\n",
      "\n",
      "Query: 'Best programming language for data analysis'\n",
      "------------------------------------------------------------\n",
      "Best match: Python is a popular programming language for data science....\n",
      "Similarity: 0.6565\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What causes global warming?\",\n",
    "    \"How does AI work?\",\n",
    "    \"Best programming language for data analysis\"\n",
    "]\n",
    "\n",
    "print(\"Comparing different queries:\\n\")\n",
    "\n",
    "for query in queries:\n",
    "    query_vec = embedder.embed_query(query)\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Find most similar document\n",
    "    similarities = [cosine_similarity(query_vec, doc_vec) for doc_vec in doc_vectors]\n",
    "    best_match_idx = np.argmax(similarities)\n",
    "    \n",
    "    print(f\"Best match: {documents[best_match_idx][:60]}...\")\n",
    "    print(f\"Similarity: {similarities[best_match_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Get Embeddings Instance\n",
    "\n",
    "Test the `get_embeddings()` method (used by vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embeddings instance type: <class 'langchain_community.embeddings.huggingface.HuggingFaceEmbeddings'>\n",
      "✓ Model name: sentence-transformers/all-MiniLM-L6-v2\n",
      "✓ This instance will be passed to ChromaVectorStore\n"
     ]
    }
   ],
   "source": [
    "embeddings_instance = embedder.get_embeddings()\n",
    "print(f\"✓ Embeddings instance type: {type(embeddings_instance)}\")\n",
    "print(f\"✓ Model name: {embeddings_instance.model_name}\")\n",
    "print(f\"✓ This instance will be passed to ChromaVectorStore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7: Performance Check\n",
    "\n",
    "Measure embedding speed for different batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance test:\n",
      "\n",
      "Single query: 0.0649 seconds\n",
      "Batch of 10: 0.0715 seconds (0.0071 per doc)\n",
      "Batch of 100: 0.5873 seconds (0.0059 per doc)\n",
      "\n",
      "✓ Batch processing is more efficient!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Test with different batch sizes\n",
    "test_texts = [f\"This is test document number {i}\" for i in range(100)]\n",
    "\n",
    "print(\"Performance test:\\n\")\n",
    "\n",
    "# Single query\n",
    "start = time.time()\n",
    "_ = embedder.embed_query(test_texts[0])\n",
    "single_time = time.time() - start\n",
    "print(f\"Single query: {single_time:.4f} seconds\")\n",
    "\n",
    "# Batch of 10\n",
    "start = time.time()\n",
    "_ = embedder.embed_documents(test_texts[:10])\n",
    "batch_10_time = time.time() - start\n",
    "print(f\"Batch of 10: {batch_10_time:.4f} seconds ({batch_10_time/10:.4f} per doc)\")\n",
    "\n",
    "# Batch of 100\n",
    "start = time.time()\n",
    "_ = embedder.embed_documents(test_texts)\n",
    "batch_100_time = time.time() - start\n",
    "print(f\"Batch of 100: {batch_100_time:.4f} seconds ({batch_100_time/100:.4f} per doc)\")\n",
    "\n",
    "print(\"\\n✓ Batch processing is more efficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Tested:\n",
    "1. ✅ Initialize EmbeddingsGenerator with default model\n",
    "2. ✅ Embed single queries with `embed_query()`\n",
    "3. ✅ Embed multiple documents with `embed_documents()`\n",
    "4. ✅ Verify vector dimensions (384 for all-MiniLM-L6-v2)\n",
    "5. ✅ Demonstrate semantic similarity\n",
    "6. ✅ Get embeddings instance for vectorstore\n",
    "7. ✅ Check performance characteristics\n",
    "\n",
    "### Key Findings:\n",
    "- Model runs locally (no API calls needed)\n",
    "- Produces 384-dimensional normalized vectors\n",
    "- Understands semantic meaning (climate queries match climate docs)\n",
    "- Batch processing is more efficient than individual queries\n",
    "- Ready to integrate with ChromaVectorStore\n",
    "\n",
    "### Next Step:\n",
    "Test the ChromaVectorStore in notebook `04_vectorstore_test.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

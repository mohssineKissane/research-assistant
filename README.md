# Research Assistant

A personal research assistant that answers questions from uploaded PDFs and the live web. Built with LangChain, Groq, and Streamlit.

üåê **Live Demo:** [research-assistantgit-mmgpmwqhp9bnhhpzy64yrh.streamlit.app](https://research-assistantgit-mmgpmwqhp9bnhhpzy64yrh.streamlit.app/)

---

## üéØ Two Modes

### üîπ Simple Mode (RAG)
Fast, document-focused conversational Q&A.
- Always retrieves from your uploaded PDFs
- Answers with source citations (document + page number)
- Remembers conversation history
- Best for: questions where the answer is in your documents

### ü§ñ Agent Mode (RAG + Web + Summarization) ‚Äî Default
Autonomous research agent using the ReAct pattern (Reasoning + Acting).
- Decides which tool to use based on the question
- Searches uploaded PDFs when the answer is there
- Searches the live web (Tavily) for current events or missing information
- Summarizes document content on request
- Remembers conversation history
- Best for: complex research requiring multiple sources or up-to-date information

---

## üöÄ Features

- **PDF upload & indexing** ‚Äî Upload one or more PDFs; they are chunked, embedded, and indexed in ChromaDB
- **Semantic search** ‚Äî Questions are matched to the most relevant document chunks
- **Source citations** ‚Äî Every document answer includes the source file and page number
- **Conversational memory** ‚Äî Multi-turn chat; the assistant understands follow-up questions
- **Live web search** ‚Äî Tavily API fetches current information the documents don't contain
- **Session management** ‚Äî Create and switch between multiple conversation sessions
- **Streamlit UI** ‚Äî Clean chat interface with sidebar settings

---

## üõ†Ô∏è Tech Stack

| Component | Technology |
|-----------|------------|
| **LLM** | Groq ‚Äî `llama-3.3-70b-versatile` |
| **Embeddings** | HuggingFace `all-MiniLM-L6-v2` (runs locally) |
| **Vector DB** | ChromaDB (local) |
| **Web Search** | Tavily API |
| **Framework** | LangChain (chains, agents, memory) |
| **UI** | Streamlit |
| **Doc Processing** | PyPDF |
| **Package Manager** | uv |

---

## üèóÔ∏è Architecture

```
User (Streamlit UI)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Simple Mode          Agent Mode        ‚îÇ
‚îÇ  (Conversational      (ReAct Agent)     ‚îÇ
‚îÇ   RAG Chain)              ‚Üì             ‚îÇ
‚îÇ       ‚Üì           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ       ‚îÇ           ‚Üì      ‚Üì      ‚Üì       ‚îÇ
‚îÇ       ‚îÇ      Doc Search  Web  Summarize ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì           ‚Üì      ‚Üì      ‚Üì
    ChromaDB    ChromaDB Tavily ChromaDB
        ‚Üì
  LLM (Groq llama-3.3-70b-versatile)
        ‚Üì
  Conversation Memory
        ‚Üì
  Response with citations ‚Üí User
```

---

## üìÇ Project Structure

```
research-assistant/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agent/              # ResearchAgent (ReAct) + AgentConfig
‚îÇ   ‚îú‚îÄ‚îÄ tools/              # document_search, web_search, summarization
‚îÇ   ‚îú‚îÄ‚îÄ chains/             # conversational.py, retrieval_qa.py
‚îÇ   ‚îú‚îÄ‚îÄ processing/         # PDF loader, text splitter, embeddings pipeline
‚îÇ   ‚îú‚îÄ‚îÄ vectorstore/        # ChromaDB wrapper
‚îÇ   ‚îú‚îÄ‚îÄ memory/             # ConversationMemoryManager
‚îÇ   ‚îî‚îÄ‚îÄ utils/              # config, llm, prompts, formatters
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ streamlit_app.py    # Main entry point
‚îÇ   ‚îú‚îÄ‚îÄ components/         # chat_interface, sidebar, document_viewer
‚îÇ   ‚îî‚îÄ‚îÄ utils/              # state_manager, ui_helpers
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ temp_uploads/       # Uploaded PDFs (session)
‚îÇ   ‚îî‚îÄ‚îÄ vectorstore/        # ChromaDB index
‚îú‚îÄ‚îÄ notebooks/              # Jupyter experiments
‚îú‚îÄ‚îÄ .streamlit/
‚îÇ   ‚îî‚îÄ‚îÄ config.toml         # Streamlit server + theme config
‚îú‚îÄ‚îÄ config.yaml             # LLM, embeddings, vectorstore settings
‚îú‚îÄ‚îÄ requirements.txt        # Production dependencies
‚îú‚îÄ‚îÄ pyproject.toml          # Project metadata (uv)
‚îî‚îÄ‚îÄ .env                    # API keys (local only, not committed)
```

---

## üì¶ Local Setup

### Prerequisites
- Python 3.11
- [uv](https://github.com/astral-sh/uv) package manager
- [Groq API key](https://console.groq.com/keys) (free)
- [Tavily API key](https://tavily.com) (free ‚Äî 1000 searches/month)

### Steps

```bash
# 1. Clone
git clone https://github.com/yourusername/research-assistant.git
cd research-assistant

# 2. Install dependencies
uv sync

# 3. Create .env
echo GROQ_API_KEY=your_key_here >> .env
echo TAVILY_API_KEY=your_key_here >> .env

# 4. Run
uv run streamlit run app/streamlit_app.py
```

---

## ‚òÅÔ∏è Deployment

Deployed on **Streamlit Community Cloud**.

üåê [research-assistantgit-mmgpmwqhp9bnhhpzy64yrh.streamlit.app](https://research-assistantgit-mmgpmwqhp9bnhhpzy64yrh.streamlit.app/)

To deploy your own instance:
1. Push this repo to GitHub
2. Go to [share.streamlit.io](https://share.streamlit.io) ‚Üí New app
3. Set main file: `app/streamlit_app.py`
4. Add secrets (`GROQ_API_KEY`, `TAVILY_API_KEY`) in App Settings ‚Üí Secrets

> **Note:** Streamlit Cloud has no persistent storage. Uploaded PDFs and the vector store reset on each restart ‚Äî users need to re-upload documents per session.

---

## ‚ö†Ô∏è Limitations

- PDF files only (no Word, Excel, etc.)
- Single-user (no authentication)
- No persistent storage on Streamlit Cloud (re-upload needed after restart)
- Groq free tier has rate limits (TPM/RPM)

# Research Assistant Configuration File
# This file centralizes all configuration for LLM, embeddings, and document processing

# LLM Configuration
llm:
  model_name: "llama-3.3-70b-versatile"  # Better quality and higher context limit
  # Alternative: "llama-3.1-8b-instant" (faster, but 6000 TPM limit causes errors with large tool outputs)
  temperature: 0.7
  max_tokens: 1024

# Embeddings Configuration
embeddings:
  model_name: "sentence-transformers/all-MiniLM-L6-v2"
  device: "cpu"  # Use 'cuda' if GPU available
  normalize: true

# Vector Store Configuration
vectorstore:
  chunk_size: 1000
  chunk_overlap: 200
  persist_directory: "./data/vectorstore"

# Web Search Configuration
web_search:
  provider: "tavily"  # Using Tavily - designed for AI agents
  max_results: 3      # Reduced from 5 to limit token usage
  # Tavily API (get free key at https://tavily.com)
  tavily_api_key: "${TAVILY_API_KEY}"
